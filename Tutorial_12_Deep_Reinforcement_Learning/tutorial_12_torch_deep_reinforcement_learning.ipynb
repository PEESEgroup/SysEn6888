{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p62G8M_viUJp"
   },
   "source": [
    "# Deep Reinforcement Learning\n",
    "\n",
    "- Playing CartPole with the Actor-Critic Method\n",
    "- Deep Deterministic Policy Gradient (DDPG)for the classic Inverted Pendulum control problem\n",
    "- Deep Q-Learning for Atari Breakout\n",
    "- Proximal Policy Optimization\n",
    "\n",
    "\n",
    "# Playing CartPole with the Actor-Critic Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFgN7h_wiUJq"
   },
   "source": [
    "This tutorial demonstrates how to implement the [Actor-Critic](https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf) method to train an agent on the [Open AI Gym](https://gym.openai.com/) CartPole-V0 environment.\n",
    "This is built on [policy gradient methods](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf) of reinforcement learning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kA10ZKRR0hi"
   },
   "source": [
    "**Actor-Critic methods**\n",
    "\n",
    "Actor-Critic methods are [temporal difference (TD) learning](https://en.wikipedia.org/wiki/Temporal_difference_learning) methods that represent the policy function independent of the value function. \n",
    "\n",
    "A policy function (or policy) returns a probability distribution over actions that the agent can take based on the given state.\n",
    "A value function determines the expected return for an agent starting at a given state and acting according to a particular policy forever after.\n",
    "\n",
    "In the Actor-Critic method, the policy is referred to as the *actor* that proposes a set of possible actions given a state, and the estimated value function is referred to as the *critic*, which evaluates actions taken by the *actor* based on the given policy.\n",
    "\n",
    "In this tutorial, both the *Actor* and *Critic* will be represented using one neural network with two outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBfiafKSRs2k"
   },
   "source": [
    "**CartPole-v0**\n",
    "\n",
    "In the [CartPole-v0 environment](https://gym.openai.com/envs/CartPole-v0), a pole is attached to a cart moving along a frictionless track. \n",
    "The pole starts upright and the goal of the agent is to prevent it from falling over by applying a force of -1 or +1 to the cart. \n",
    "A reward of +1 is given for every time step the pole remains upright.\n",
    "An episode ends when (1) the pole is more than 15 degrees from vertical or (2) the cart moves more than 2.4 units from the center.\n",
    "\n",
    "![title](https://www.tensorflow.org/tutorials/reinforcement_learning/images/cartpole-v0.gif)\n",
    "\n",
    "<center>\n",
    "  <figure>\n",
    "    <image src=\"https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic_files/output_TLd720SejKmf_0.gif\">\n",
    "    <figcaption>\n",
    "      Trained actor-critic model in Cartpole-v0 environment\n",
    "    </figcaption>\n",
    "  </figure>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSNVK0AeRoJd"
   },
   "source": [
    "The problem is considered \"solved\" when the average total reward for the episode reaches 195 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glLwIctHiUJq"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary packages and configure global settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "13l6BbxKhCKp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /opt/anaconda3/lib/python3.12/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /opt/anaconda3/lib/python3.12/site-packages (from gym) (0.0.8)\n",
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp312-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.20.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.5.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.5.1-cp312-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Downloading torchvision-0.20.1-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.5.1-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sympy, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "Successfully installed sympy-1.13.1 torch-2.5.1 torchaudio-2.5.1 torchvision-0.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WBeQhPi2S4m5"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install additional packages for visualization\n",
    "# sudo apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
    "pip install pyvirtualdisplay > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tT4N3qYviUJr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "env.reset(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOUCe2D0iUJu"
   },
   "source": [
    "## Model\n",
    "\n",
    "The *Actor* and *Critic* will be modeled using one neural network that generates the action probabilities and critic value respectively. This tutorial uses model subclassing to define the model. \n",
    "\n",
    "During the forward pass, the model will take in the state as the input and will output both action probabilities and critic value $V$, which models the state-dependent [value function](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions). The goal is to train a model that chooses actions based on a policy $\\pi$ that maximizes expected [return](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#reward-and-return).\n",
    "\n",
    "For Cartpole-v0, there are four values representing the state: cart position, cart-velocity, pole angle and pole velocity respectively. The agent can take two actions to push the cart left (0) and right (1) respectively.\n",
    "\n",
    "Refer to [OpenAI Gym's CartPole-v0 wiki page](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf) for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aXKbbMC-kmuv"
   },
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.common = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(hidden_size, num_actions)\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.common(x)\n",
    "        return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nWyxJgjLn68c"
   },
   "outputs": [],
   "source": [
    "num_inputs = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "hidden_size = 128\n",
    "\n",
    "model = ActorCritic(num_inputs, num_actions, hidden_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk92njFziUJw"
   },
   "source": [
    "## Training\n",
    "\n",
    "To train the agent, you will follow these steps:\n",
    "\n",
    "1. Run the agent on the environment to collect training data per episode.\n",
    "2. Compute expected return at each time step.\n",
    "3. Compute the loss for the combined actor-critic model.\n",
    "4. Compute gradients and update network parameters.\n",
    "5. Repeat 1-4 until either success criterion or max episodes has been reached.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2nde2XDs8Gh"
   },
   "source": [
    "### 1. Collecting training data\n",
    "\n",
    "As in supervised learning, in order to train the actor-critic model, you need\n",
    "to have training data. However, in order to collect such data, the model would\n",
    "need to be \"run\" in the environment.\n",
    "\n",
    "Training data is collected for each episode. Then at each time step, the model's forward pass will be run on the environment's state in order to generate action probabilities and the critic value based on the current policy parameterized by the model's weights.\n",
    "\n",
    "The next action will be sampled from the action probabilities generated by the model, which would then be applied to the environment, causing the next state and reward to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5URrbGlDSAGx"
   },
   "outputs": [],
   "source": [
    "def env_step(action):\n",
    "    state_info = env.step(action)\n",
    "    if isinstance(state_info, tuple):\n",
    "        state, reward, done, _ = state_info\n",
    "    else:\n",
    "        state, reward, done = state_info\n",
    "    return torch.tensor(state, dtype=torch.float32), reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "a4qVRV063Cl9"
   },
   "outputs": [],
   "source": [
    "def run_episode(model, max_steps):\n",
    "    state_info = env.reset()\n",
    "    state = torch.tensor(state_info[0] if isinstance(state_info, tuple) else state_info, dtype=torch.float32)\n",
    "    log_probs, values, rewards = [], [], []\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        logits, value = model(state)\n",
    "        action_probs = torch.softmax(logits, dim=-1)\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "\n",
    "        # Append log probabilities and values\n",
    "        log_probs.append(torch.log(action_probs[action]).unsqueeze(0))\n",
    "        values.append(value)\n",
    "\n",
    "        state_info = env.step(action)\n",
    "        if isinstance(state_info, tuple):\n",
    "            state, reward, done, *_ = state_info\n",
    "        else:\n",
    "            state, reward, done = state_info\n",
    "\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return log_probs, values, rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBnIHdz22dIx"
   },
   "source": [
    "### 2. Computing expected returns\n",
    "\n",
    "The sequence of rewards for each timestep $t$, $\\{r_{t}\\}^{T}_{t=1}$ collected during one episode is converted into a sequence of expected returns $\\{G_{t}\\}^{T}_{t=1}$ in which the sum of rewards is taken from the current timestep $t$ to $T$ and each reward is multiplied with an exponentially decaying discount factor $\\gamma$:\n",
    "\n",
    "$$G_{t} = \\sum^{T}_{t'=t} \\gamma^{t'-t}r_{t'}$$\n",
    "\n",
    "Since $\\gamma\\in(0,1)$, rewards further out from the current timestep are given less weight.\n",
    "\n",
    "Intuitively, expected return simply implies that rewards now are better than rewards later. In a mathematical sense, it is to ensure that the sum of the rewards converges.\n",
    "\n",
    "To stabilize training, the resulting sequence of returns is also standardized (i.e. to have zero mean and unit standard deviation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jpEwFyl315dl"
   },
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    returns = []\n",
    "    discounted_sum = 0\n",
    "    for reward in reversed(rewards):\n",
    "        discounted_sum = reward + gamma * discounted_sum\n",
    "        returns.insert(0, discounted_sum)\n",
    "\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    return (returns - returns.mean()) / (returns.std() + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hrPLrgGxlvb"
   },
   "source": [
    "### 3. The actor-critic loss\n",
    "\n",
    "Since a hybrid actor-critic model is used, the chosen loss function is a combination of actor and critic losses for training, as shown below:\n",
    "\n",
    "$$L = L_{actor} + L_{critic}$$\n",
    "\n",
    "#### Actor loss\n",
    "\n",
    "The actor loss is based on [policy gradients with the critic as a state dependent baseline](https://www.youtube.com/watch?v=EKqxumCuAAY&t=62m23s) and computed with single-sample (per-episode) estimates.\n",
    "\n",
    "$$L_{actor} = -\\sum^{T}_{t=1} log\\pi_{\\theta}(a_{t} | s_{t})[G(s_{t}, a_{t})  - V^{\\pi}_{\\theta}(s_{t})]$$\n",
    "\n",
    "where:\n",
    "- $T$: the number of timesteps per episode, which can vary per episode\n",
    "- $s_{t}$: the state at timestep $t$\n",
    "- $a_{t}$: chosen action at timestep $t$ given state $s$\n",
    "- $\\pi_{\\theta}$: is the policy (actor) parameterized by $\\theta$\n",
    "- $V^{\\pi}_{\\theta}$: is the value function (critic) also parameterized by $\\theta$\n",
    "- $G = G_{t}$: the expected return for a given state, action pair at timestep $t$\n",
    "\n",
    "A negative term is added to the sum since the idea is to maximize the probabilities of actions yielding higher rewards by minimizing the combined loss.\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Advantage\n",
    "\n",
    "The $G - V$ term in our $L_{actor}$ formulation is called the [advantage](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions), which indicates how much better an action is given a particular state over a random action selected according to the policy $\\pi$ for that state.\n",
    "\n",
    "While it's possible to exclude a baseline, this may result in high variance during training. And the nice thing about choosing the critic $V$ as a baseline is that it trained to be as close as possible to $G$, leading to a lower variance.\n",
    "\n",
    "In addition, without the critic, the algorithm would try to increase probabilities for actions taken on a particular state based on expected return, which may not make much of a difference if the relative probabilities between actions remain the same.\n",
    "\n",
    "For instance, suppose that two actions for a given state would yield the same expected return. Without the critic, the algorithm would try to raise the probability of these actions based on the objective $J$. With the critic, it may turn out that there's no advantage ($G - V = 0$) and thus no benefit gained in increasing the actions' probabilities and the algorithm would set the gradients to zero.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Critic loss\n",
    "\n",
    "Training $V$ to be as close possible to $G$ can be set up as a regression problem with the following loss function:\n",
    "\n",
    "$$L_{critic} = L_{\\delta}(G, V^{\\pi}_{\\theta})$$\n",
    "\n",
    "where $L_{\\delta}$ is the [Huber loss](https://en.wikipedia.org/wiki/Huber_loss), which is less sensitive to outliers in data than squared-error loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9EXwbEez6n9m"
   },
   "outputs": [],
   "source": [
    "def compute_loss(log_probs, values, returns):\n",
    "    advantage = returns - torch.cat(values)\n",
    "    actor_loss = -torch.sum(torch.cat(log_probs) * advantage.detach())\n",
    "    critic_loss = advantage.pow(2).mean()\n",
    "    return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSYkQOmRfV75"
   },
   "source": [
    "### 4. Defining the training step to update parameters\n",
    "\n",
    "All of the steps above are combined into a training step that is run every episode. All steps leading up to the loss function are executed with the `tf.GradientTape` context to enable automatic differentiation.\n",
    "\n",
    "This tutorial uses the Adam optimizer to apply the gradients to the model parameters.\n",
    "\n",
    "The sum of the undiscounted rewards, `episode_reward`, is also computed in this step. This value will be used later on to evaluate if the success criterion is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QoccrkF3IFCg"
   },
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, gamma=0.99, max_steps_per_episode=1000):\n",
    "    log_probs, values, rewards = run_episode(model, max_steps_per_episode)\n",
    "    returns = compute_returns(rewards, gamma)\n",
    "\n",
    "    loss = compute_loss(log_probs, values, returns)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFvZiDoAflGK"
   },
   "source": [
    "### 5. Run the training loop\n",
    "\n",
    "Training is executed by running the training step until either the success criterion or maximum number of episodes is reached.  \n",
    "\n",
    "A running record of episode rewards is kept in a queue. Once 100 trials are reached, the oldest reward is removed at the left (tail) end of the queue and the newest one is added at the head (right). A running sum of the rewards is also maintained for computational efficiency. \n",
    "\n",
    "Depending on your runtime, training can finish in less than a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "kbmBxnzLiUJx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved at episode 155: average reward: 196.48!\n"
     ]
    }
   ],
   "source": [
    "min_episodes_criterion = 100\n",
    "max_episodes = 10000\n",
    "reward_threshold = 195\n",
    "running_reward = 0\n",
    "\n",
    "episode_rewards = deque(maxlen=min_episodes_criterion)\n",
    "\n",
    "for i in range(max_episodes):\n",
    "    episode_reward = train_step(model, optimizer)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    running_reward = np.mean(episode_rewards)\n",
    "\n",
    "    if running_reward > reward_threshold and i >= min_episodes_criterion:\n",
    "        print(f'Solved at episode {i}: average reward: {running_reward:.2f}!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ru8BEwS1EmAv"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "After training, it would be good to visualize how the model performs in the environment. You can run the cells below to generate a GIF animation of one episode run of the model. Note that additional packages need to be installed for OpenAI Gym to render the environment's images correctly in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvirtualdisplay in /opt/anaconda3/lib/python3.12/site-packages (3.0)\n",
      "Requirement already satisfied: xvfbwrapper in /opt/anaconda3/lib/python3.12/site-packages (0.2.9)\n",
      "zsh:1: no matches found: gym[classic_control]\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (4.66.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyvirtualdisplay xvfbwrapper\n",
    "!pip install gym[classic_control]\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "qbIMMkfmRHyC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "def render_episode(env, model, max_steps):\n",
    "    state = env.reset()\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        frame = env.render()\n",
    "        frames.append(Image.fromarray(frame))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(state.unsqueeze(0))[0]\n",
    "        action = torch.argmax(torch.softmax(logits, dim=-1)).item()\n",
    "\n",
    "        state, _, done, *_ = env.step(action)\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return frames\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "frames = render_episode(env, model, max_steps=1000)\n",
    "frames[0].save('cartpole.gif', save_all=True, append_images=frames[1:], loop=0, duration=40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic_files/output_TLd720SejKmf_0.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient (DDPG)for the classic Inverted Pendulum control problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Deep Deterministic Policy Gradient (DDPG) is a model-free off-policy algorithm for learning continous actions.\n",
    "\n",
    "It combines ideas from DPG (Deterministic Policy Gradient) and DQN (Deep Q-Network). It uses Experience Replay and slow-learning target networks from DQN, and it is based on DPG, which can operate over continuous action spaces.\n",
    "\n",
    "This tutorial closely follow this paper - Continuous control with deep reinforcement learning https://arxiv.org/pdf/1509.02971.pdf\n",
    "\n",
    "\n",
    "## Problem\n",
    "\n",
    "We are trying to solve the classic Inverted Pendulum control problem. In this setting, we can take only two actions: swing left or swing right.\n",
    "\n",
    "What make this problem challenging for Q-Learning Algorithms is that actions are continuous instead of being discrete. That is, instead of using two discrete actions like -1 or +1, we have to select from infinite actions ranging from -2 to +2.\n",
    "\n",
    "## Quick theory\n",
    "Just like the Actor-Critic method, we have two networks:\n",
    "\n",
    "Actor - It proposes an action given a state.\n",
    "Critic - It predicts if the action is good (positive value) or bad (negative value) given a state and an action.\n",
    "DDPG uses two more techniques not present in the original DQN:\n",
    "\n",
    "First, it uses two Target networks.\n",
    "\n",
    "Why? Because it add stability to training. In short, we are learning from estimated targets and Target networks are updated slowly, hence keeping our estimated targets stable.\n",
    "\n",
    "Conceptually, this is like saying, \"I have an idea of how to play this well, I'm going to try it out for a bit until I find something better\", as opposed to saying \"I'm going to re-learn how to play this entire game after every move\". See this StackOverflow answer.\n",
    "\n",
    "Second, it uses Experience Replay.\n",
    "\n",
    "We store list of tuples (state, action, reward, next_state), and instead of learning only from recent experience, we learn from sampling all of our experience accumulated so far.\n",
    "\n",
    "Now, let's see how is it implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use OpenAIGym to create the environment. We will use the upper_bound parameter to scale our actions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: 3\n",
      "Action space: 1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"State space:\", num_states)\n",
    "print(\"Action space:\", num_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement better exploration by the Actor network, we use noisy perturbations, specifically an Ornstein-Uhlenbeck process for generating noise, as described in the paper. It samples noise from a correlated normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * torch.sqrt(torch.tensor(self.dt)) * torch.randn_like(self.mean)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x_initial if self.x_initial is not None else torch.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Buffer class implements Experience Replay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://i.imgur.com/mS6iGyJ.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Critic loss** - Mean Squared Error of y - Q(s, a) where y is the expected return as seen by the Target network, and Q(s, a) is action value predicted by the Critic network. y is a moving target that the critic model tries to achieve; we make this target stable by updating the Target model slowly.\n",
    "\n",
    "**Actor loss** - This is computed using the mean of the value given by the Critic network for the actions taken by the Actor network. We seek to maximize this quantity.\n",
    "\n",
    "Hence we update the Actor network so that it produces actions that get the maximum predicted value as seen by the Critic, for a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    def sample(self):\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = torch.tensor(self.state_buffer[batch_indices], dtype=torch.float32)\n",
    "        action_batch = torch.tensor(self.action_buffer[batch_indices], dtype=torch.float32)\n",
    "        reward_batch = torch.tensor(self.reward_buffer[batch_indices], dtype=torch.float32)\n",
    "        next_state_batch = torch.tensor(self.next_state_buffer[batch_indices], dtype=torch.float32)\n",
    "\n",
    "        return state_batch, action_batch, reward_batch, next_state_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the Actor and Critic networks. These are basic Dense models with ReLU activation.\n",
    "\n",
    "Note: We need the initialization for last layer of the Actor to be between -0.003 and 0.003 as this prevents us from getting 1 or -1 output values in the initial stages, which would squash our gradients to zero, as we use the tanh activation.\n",
    "\n",
    "def get_actor():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_bound):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.out = nn.Linear(256, action_dim)\n",
    "        self.action_bound = action_bound\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.out(x)) * self.action_bound\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.out = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(num_states, num_actions, upper_bound)\n",
    "critic = Critic(num_states, num_actions)\n",
    "\n",
    "actor_target = Actor(num_states, num_actions, upper_bound)\n",
    "critic_target = Critic(num_states, num_actions)\n",
    "\n",
    "actor_target.load_state_dict(actor.state_dict())\n",
    "critic_target.load_state_dict(critic.state_dict())\n",
    "\n",
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=0.001)\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement our main training loop, and iterate over episodes. We sample actions using policy() and train with learn() at each time step, along with updating the Target networks at a rate tau.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 1/100 [00:01<01:43,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Avg Reward: -7928.39304918569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 2/100 [00:02<01:47,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Avg Reward: -8035.720304903359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 3/100 [00:03<01:47,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2, Avg Reward: -6724.8359694201345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 4/100 [00:04<01:48,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3, Avg Reward: -5553.987657365215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 5/100 [00:05<01:48,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4, Avg Reward: -5850.602330894841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 6/100 [00:06<01:45,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5, Avg Reward: -5199.056263391598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 7/100 [00:07<01:44,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6, Avg Reward: -4680.25564028557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 8/100 [00:08<01:43,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7, Avg Reward: -4220.794811854083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|▉         | 9/100 [00:10<01:42,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8, Avg Reward: -3756.4300920428204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|█         | 10/100 [00:11<01:41,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9, Avg Reward: -3393.1288136561066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  11%|█         | 11/100 [00:12<01:40,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Avg Reward: -3096.50481498606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  12%|█▏        | 12/100 [00:13<01:39,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11, Avg Reward: -2849.1355686091597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  13%|█▎        | 13/100 [00:14<01:37,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12, Avg Reward: -2649.4174341704947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  14%|█▍        | 14/100 [00:15<01:36,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13, Avg Reward: -2469.2121566231485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  15%|█▌        | 15/100 [00:16<01:35,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14, Avg Reward: -2328.4950018078957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  16%|█▌        | 16/100 [00:17<01:34,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15, Avg Reward: -2191.436879246178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  17%|█▋        | 17/100 [00:19<01:34,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16, Avg Reward: -2069.726357174769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  18%|█▊        | 18/100 [00:20<01:32,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17, Avg Reward: -1962.6947384066652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  19%|█▉        | 19/100 [00:21<01:31,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18, Avg Reward: -1867.5306238497114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|██        | 20/100 [00:22<01:30,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19, Avg Reward: -1793.3796281528425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  21%|██        | 21/100 [00:23<01:29,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20, Avg Reward: -1720.5123535308517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  22%|██▏       | 22/100 [00:24<01:28,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 21, Avg Reward: -1643.8754120521598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  23%|██▎       | 23/100 [00:25<01:27,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 22, Avg Reward: -1583.8444952291811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  24%|██▍       | 24/100 [00:27<01:26,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 23, Avg Reward: -1557.841254465067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  25%|██▌       | 25/100 [00:28<01:25,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 24, Avg Reward: -1496.0453007354333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  26%|██▌       | 26/100 [00:29<01:24,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25, Avg Reward: -1452.7319183933525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  27%|██▋       | 27/100 [00:30<01:23,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 26, Avg Reward: -1408.5879104361088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  28%|██▊       | 28/100 [00:31<01:23,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 27, Avg Reward: -1380.9637797066648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  29%|██▉       | 29/100 [00:32<01:24,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 28, Avg Reward: -1341.785353821872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  30%|███       | 30/100 [00:34<01:24,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 29, Avg Reward: -1301.2565247163916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  31%|███       | 31/100 [00:35<01:26,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 30, Avg Reward: -1267.1140659584923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  32%|███▏      | 32/100 [00:36<01:23,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 31, Avg Reward: -1231.4227737857636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  33%|███▎      | 33/100 [00:37<01:22,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 32, Avg Reward: -1194.3569798536869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  34%|███▍      | 34/100 [00:39<01:19,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 33, Avg Reward: -1163.1105505823284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  35%|███▌      | 35/100 [00:40<01:16,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 34, Avg Reward: -1130.138285287966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  36%|███▌      | 36/100 [00:41<01:14,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 35, Avg Reward: -1102.4434915480986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  37%|███▋      | 37/100 [00:42<01:12,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 36, Avg Reward: -1073.0908178989575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  38%|███▊      | 38/100 [00:43<01:11,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 37, Avg Reward: -1048.2742192405155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  39%|███▉      | 39/100 [00:44<01:10,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 38, Avg Reward: -1028.2305378564236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  40%|████      | 40/100 [00:46<01:11,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 39, Avg Reward: -1005.8312310005192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  41%|████      | 41/100 [00:47<01:11,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 40, Avg Reward: -810.8535567719118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  42%|████▏     | 42/100 [00:48<01:10,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 41, Avg Reward: -617.7397940327189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  43%|████▎     | 43/100 [00:49<01:08,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 42, Avg Reward: -518.7217643417325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  44%|████▍     | 44/100 [00:50<01:06,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 43, Avg Reward: -487.29075113425245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  45%|████▌     | 45/100 [00:52<01:07,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 44, Avg Reward: -315.0948287293013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  46%|████▌     | 46/100 [00:53<01:05,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 45, Avg Reward: -266.70207699530795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  47%|████▋     | 47/100 [00:54<01:03,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 46, Avg Reward: -230.7574299117342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  48%|████▊     | 48/100 [00:55<01:03,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 47, Avg Reward: -208.87779857443925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  49%|████▉     | 49/100 [00:57<01:02,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 48, Avg Reward: -215.82829322365734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  50%|█████     | 50/100 [00:58<01:01,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 49, Avg Reward: -216.23279405956356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  51%|█████     | 51/100 [00:59<01:01,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, Avg Reward: -216.39668536033992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  52%|█████▏    | 52/100 [01:00<01:01,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 51, Avg Reward: -213.28349701207284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  53%|█████▎    | 53/100 [01:02<01:01,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 52, Avg Reward: -210.49984280137514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  54%|█████▍    | 54/100 [01:03<01:00,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 53, Avg Reward: -217.0843729701588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  55%|█████▌    | 55/100 [01:04<00:57,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 54, Avg Reward: -211.53076497524717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  56%|█████▌    | 56/100 [01:06<00:55,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 55, Avg Reward: -214.58575604454896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  57%|█████▋    | 57/100 [01:07<00:52,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 56, Avg Reward: -214.79373493161066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  58%|█████▊    | 58/100 [01:08<00:51,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 57, Avg Reward: -214.57786104616426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  59%|█████▉    | 59/100 [01:09<00:50,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 58, Avg Reward: -220.27838513742887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  60%|██████    | 60/100 [01:10<00:48,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 59, Avg Reward: -216.8783489353504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  61%|██████    | 61/100 [01:12<00:47,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 60, Avg Reward: -213.72383230942108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  62%|██████▏   | 62/100 [01:13<00:46,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 61, Avg Reward: -288.0456287468555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  63%|██████▎   | 63/100 [01:14<00:45,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 62, Avg Reward: -287.61744160751243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  64%|██████▍   | 64/100 [01:15<00:44,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 63, Avg Reward: -266.7321286691813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  65%|██████▌   | 65/100 [01:17<00:43,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 64, Avg Reward: -266.53258296016423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  66%|██████▌   | 66/100 [01:18<00:42,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 65, Avg Reward: -257.39353385268276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  67%|██████▋   | 67/100 [01:19<00:40,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 66, Avg Reward: -257.8912700530236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  68%|██████▊   | 68/100 [01:20<00:39,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 67, Avg Reward: -275.82145875986174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  69%|██████▉   | 69/100 [01:21<00:37,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 68, Avg Reward: -275.76859043084676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  70%|███████   | 70/100 [01:23<00:36,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 69, Avg Reward: -278.41476544223644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  71%|███████   | 71/100 [01:24<00:35,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 70, Avg Reward: -275.4764084768566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  72%|███████▏  | 72/100 [01:25<00:34,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 71, Avg Reward: -278.34388714650606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  73%|███████▎  | 73/100 [01:26<00:33,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 72, Avg Reward: -281.49308432139344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  74%|███████▍  | 74/100 [01:28<00:31,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 73, Avg Reward: -281.5562769391919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  75%|███████▌  | 75/100 [01:29<00:30,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 74, Avg Reward: -290.90765967285023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  76%|███████▌  | 76/100 [01:30<00:29,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 75, Avg Reward: -290.7560765636028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  77%|███████▋  | 77/100 [01:31<00:28,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 76, Avg Reward: -296.728950229162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  78%|███████▊  | 78/100 [01:32<00:26,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 77, Avg Reward: -296.5225197135218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  79%|███████▉  | 79/100 [01:34<00:25,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 78, Avg Reward: -290.2766897185508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  80%|████████  | 80/100 [01:35<00:24,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 79, Avg Reward: -287.37003908037406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  81%|████████  | 81/100 [01:36<00:23,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 80, Avg Reward: -287.654470390893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  82%|████████▏ | 82/100 [01:37<00:22,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 81, Avg Reward: -286.74912023598336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  83%|████████▎ | 83/100 [01:39<00:20,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 82, Avg Reward: -286.760510783534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  84%|████████▍ | 84/100 [01:40<00:19,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 83, Avg Reward: -270.6036170219472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  85%|████████▌ | 85/100 [01:41<00:18,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 84, Avg Reward: -270.27836102887414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  86%|████████▌ | 86/100 [01:42<00:17,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 85, Avg Reward: -273.2767095343299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  87%|████████▋ | 87/100 [01:43<00:15,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 86, Avg Reward: -276.43466453476566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  88%|████████▊ | 88/100 [01:45<00:14,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 87, Avg Reward: -276.6098564463129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  89%|████████▉ | 89/100 [01:46<00:14,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 88, Avg Reward: -272.0005896614095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  90%|█████████ | 90/100 [01:48<00:13,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 89, Avg Reward: -274.4214071475809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  91%|█████████ | 91/100 [01:49<00:12,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 90, Avg Reward: -277.29944241200224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  92%|█████████▏| 92/100 [01:50<00:11,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 91, Avg Reward: -280.1314017746589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  93%|█████████▎| 93/100 [01:52<00:09,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 92, Avg Reward: -279.7959698197057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  94%|█████████▍| 94/100 [01:53<00:08,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 93, Avg Reward: -270.0744681000527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  95%|█████████▌| 95/100 [01:55<00:07,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 94, Avg Reward: -272.9337366456222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  96%|█████████▌| 96/100 [01:56<00:05,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 95, Avg Reward: -275.99312842983466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  97%|█████████▋| 97/100 [01:58<00:04,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 96, Avg Reward: -278.72502990525794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  98%|█████████▊| 98/100 [01:59<00:02,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 97, Avg Reward: -278.45082766447706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  99%|█████████▉| 99/100 [02:01<00:01,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 98, Avg Reward: -272.0729234089743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 100/100 [02:02<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 99, Avg Reward: -271.87327773138577\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import the tqdm library for progress display\n",
    "\n",
    "ep_reward_list = []\n",
    "avg_reward_list = []\n",
    "\n",
    "buffer = Buffer(50000, 64)\n",
    "ou_noise = OUActionNoise(mean=torch.zeros(1), std_deviation=0.2 * torch.ones(1))\n",
    "\n",
    "def policy(state, noise):\n",
    "    # Use the actor network to predict actions with added noise for exploration\n",
    "    with torch.no_grad():\n",
    "        action = actor(state) + noise()\n",
    "    return action.clamp(lower_bound, upper_bound)\n",
    "\n",
    "# Set a maximum step limit per episode to prevent infinite loops\n",
    "MAX_STEPS_PER_EPISODE = 1000\n",
    "\n",
    "# Wrap the main loop with tqdm for progress tracking\n",
    "for ep in tqdm(range(100), desc=\"Training Progress\"):\n",
    "    state = env.reset()\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]  # Unpack if the environment returns a tuple\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    episodic_reward = 0\n",
    "\n",
    "    for step in range(MAX_STEPS_PER_EPISODE):  # Add a step limit\n",
    "        # Select action using the policy and apply it to the environment\n",
    "        action = policy(state.unsqueeze(0), ou_noise).squeeze(0).numpy()\n",
    "        next_state, reward, done, *_ = env.step(action)\n",
    "\n",
    "        # Record the transition in the replay buffer\n",
    "        buffer.record((state.numpy(), action, reward, next_state))\n",
    "        state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "        # Perform training when there are enough samples in the buffer\n",
    "        if buffer.buffer_counter > buffer.batch_size:\n",
    "            state_batch, action_batch, reward_batch, next_state_batch = buffer.sample()\n",
    "\n",
    "            # Update Critic network\n",
    "            target_actions = actor_target(next_state_batch)\n",
    "            y = reward_batch + 0.99 * critic_target(next_state_batch, target_actions)\n",
    "            critic_loss = F.mse_loss(critic(state_batch, action_batch), y.detach())\n",
    "\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            # Update Actor network\n",
    "            actor_loss = -critic(state_batch, actor(state_batch)).mean()\n",
    "\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # Update target networks using soft updates\n",
    "            with torch.no_grad():\n",
    "                for target_param, param in zip(actor_target.parameters(), actor.parameters()):\n",
    "                    target_param.data.copy_(0.005 * param.data + 0.995 * target_param.data)\n",
    "\n",
    "                for target_param, param in zip(critic_target.parameters(), critic.parameters()):\n",
    "                    target_param.data.copy_(0.005 * param.data + 0.995 * target_param.data)\n",
    "\n",
    "        episodic_reward += reward\n",
    "        if done:  # Exit the loop if the episode ends\n",
    "            break\n",
    "\n",
    "    # Append the episodic reward and calculate the moving average\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "    # Print the average reward for the current episode\n",
    "    tqdm.write(f\"Episode {ep}, Avg Reward: {avg_reward}\")\n",
    "\n",
    "# Plot the average reward trend\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training proceeds correctly, the average episodic reward will increase with time.\n",
    "\n",
    "Feel free to try different learning rates, tau values, and architectures for the Actor and Critic networks.\n",
    "\n",
    "The Inverted Pendulum problem has low complexity, but DDPG work great on many other problems.\n",
    "\n",
    "Another great environment to try this on is LunarLandingContinuous-v2, but it will take more episodes to obtain good results.\n",
    "\n",
    "before training:\n",
    "![title](https://i.imgur.com/ox6b9rC.giff)\n",
    "\n",
    "after training:\n",
    "![title](https://i.imgur.com/eEH8Cz6.gif)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_jQ1tEQCxwRx"
   ],
   "name": "actor_critic.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
