# Transformers Implementation

This tutorial demonstrates how to implement a Transformer model using both TensorFlow and PyTorch. It covers all steps from data preprocessing to training and inference for a language translation task.

## Table of Contents
- Introduction
- Setup
- The Data
  - Download and Prepare the Dataset
  - Create a Dataset (`tf.data` for TensorFlow and standard PyTorch Dataset)
- Text Preprocessing
  - Standardization
  - Text Vectorization
  - Process the Dataset
- Model Components
  - The Encoder
  - The Attention Layer
  - The Decoder
- Training
  - Training the Model
  - Inference
  - Exporting the Model
- Optional
  - Using a Dynamic Loop (TensorFlow only)
- Additional Resources

## Introduction
This tutorial provides side-by-side implementations of Transformers using TensorFlow and PyTorch, showcasing their unique features and APIs.

- **TensorFlow**: Inspired by the official [TensorFlow Transformer tutorial](https://www.tensorflow.org/text/tutorials/transformer).
- **PyTorch**: Implements similar concepts using PyTorch's flexible API.

## Getting Started
1. **Environment Setup**: Ensure you have TensorFlow or PyTorch installed based on the framework you plan to use.
2. **Follow the Sections**: Start from data preparation, proceed through preprocessing, build the model, and train it.
3. **Experiment with Both Frameworks**: Compare TensorFlow's `tf.data` pipeline and PyTorch's Dataset class to understand their respective advantages.

## Highlights
- **TensorFlow**: Provides dynamic loops and seamless data pipelines using `tf.data`.
- **PyTorch**: Offers greater flexibility and is highly customizable, with a focus on hands-on control.

## Additional Resources
- TensorFlow Users: Visit the [official TensorFlow tutorials](https://www.tensorflow.org/text/tutorials) for more.
- PyTorch Users: Explore the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) for further reading.

## Acknowledgments
- Fran√ßois Chollet
- Tensorflow.org
- pytorch.org
