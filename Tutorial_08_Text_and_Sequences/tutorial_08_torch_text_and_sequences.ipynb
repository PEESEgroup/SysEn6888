{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWVYhx8E9AgL"
      },
      "source": [
        "# Text and Sequences\n",
        "\n",
        "Our tutorial today contains an introduction to word embeddings. You will train your own word embeddings using a simple model for a sentiment classification task, and then visualize them using two methods.\n",
        "\n",
        "\n",
        "## Representing text/sequnces as numbers\n",
        "\n",
        "Machine learning models take vectors (arrays of numbers) as input. When working with text, the first thing you must do is come up with a strategy to convert strings to numbers (or to \"vectorize\" the text) before feeding it to the model. In this section, you will look at three strategies for doing so.\n",
        "\n",
        "### One-hot encodings\n",
        "\n",
        "As a first idea, you might \"one-hot\" encode each word in your vocabulary. Consider the sentence \"The cat sat on the mat\". The vocabulary (or unique words) in this sentence is (cat, mat, on, sat, the). To represent each word, you will create a zero vector with length equal to the vocabulary, then place a one in the index that corresponds to the word. This approach is shown in the following diagram.\n",
        "\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/text/guide/images/one-hot.png\" alt=\"Diagram of one-hot encodings\" width=\"400\" />\n",
        "\n",
        "To create a vector that contains the encoding of the sentence, you could then concatenate the one-hot vectors for each word.\n",
        "\n",
        "Key point: This approach is inefficient. A one-hot encoded vector is sparse (meaning, most indices are zero). Imagine you have 10,000 words in the vocabulary. To one-hot encode each word, you would create a vector where 99.99% of the elements are zero.\n",
        "\n",
        "### Encode each word with a unique number\n",
        "\n",
        "A second approach you might try is to encode each word using a unique number. Continuing the example above, you could assign 1 to \"cat\", 2 to \"mat\", and so on. You could then encode the sentence \"The cat sat on the mat\" as a dense vector like [5, 1, 4, 3, 5, 2]. This approach is efficient. Instead of a sparse vector, you now have a dense one (where all elements are full).\n",
        "\n",
        "There are two downsides to this approach, however:\n",
        "\n",
        "* The integer-encoding is arbitrary (it does not capture any relationship between words).\n",
        "\n",
        "* An integer-encoding can be challenging for a model to interpret. A linear classifier, for example, learns a single weight for each feature. Because there is no relationship between the similarity of any two words and the similarity of their encodings, this feature-weight combination is not meaningful.\n",
        "\n",
        "### Word embeddings\n",
        "\n",
        "Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/text/guide/images/embedding2.png\" alt=\"Diagram of one-hot encodings\" width=\"400\" />\n",
        "\n",
        "\n",
        "Above is a diagram for a word embedding. Each word is represented as a 4-dimensional vector of floating point values. Another way to think of an embedding is as \"lookup table\". After these weights have been learned, you can encode each word by looking up the dense vector it corresponds to in the table.\n",
        "\n",
        "Let's get started with an example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AEQJPLeJ9AgM"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import shutil\n",
        "import tarfile\n",
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(123)\n",
        "np.random.seed(123)\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeR3p9yQ9AgN"
      },
      "source": [
        "### Download the IMDb Dataset\n",
        "You will use the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) through the tutorial. You will train a sentiment classifier model on this dataset and in the process learn embeddings from scratch. To read more about loading a dataset from scratch, see the [Loading text tutorial](https://www.tensorflow.org/tutorials/load_data/text).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm1k1PXE9AgN",
        "outputId": "8aaa004a-d401-49e8-e0af-7ab5116deb6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading IMDB dataset...\n",
            "Download completed.\n",
            "Extracting dataset...\n",
            "Extraction completed.\n"
          ]
        }
      ],
      "source": [
        "# Define the URL and download path\n",
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "download_path = Path('./aclImdb_v1.tar.gz')\n",
        "\n",
        "# Download the dataset if not already downloaded\n",
        "if not download_path.exists():\n",
        "    print(\"Downloading IMDB dataset...\")\n",
        "    urllib.request.urlretrieve(url, download_path)\n",
        "    print(\"Download completed.\")\n",
        "\n",
        "# Extract the dataset\n",
        "extract_path = Path('./aclImdb')\n",
        "if not extract_path.exists():\n",
        "    print(\"Extracting dataset...\")\n",
        "    with tarfile.open(download_path, 'r:gz') as tar:\n",
        "        tar.extractall(path='.')\n",
        "    print(\"Extraction completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up065Y3g9AgO"
      },
      "source": [
        "Take a look at the train/ directory. It has pos and neg folders with movie reviews labelled as positive and negative respectively. You will use reviews from pos and neg folders to train a binary classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH1AKwao9AgO",
        "outputId": "13bbe4f5-2ed7-42c7-9e76-bc2313439905",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['urls_pos.txt',\n",
              " 'unsup',\n",
              " 'labeledBow.feat',\n",
              " 'urls_neg.txt',\n",
              " 'pos',\n",
              " 'unsupBow.feat',\n",
              " 'urls_unsup.txt',\n",
              " 'neg']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_dir = './aclImdb'\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMWD4mA_9AgP"
      },
      "source": [
        "The train directory also has additional folders which should be removed before creating training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Q9Qp95Mx9AgP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removed 'unsup' directory.\n"
          ]
        }
      ],
      "source": [
        "# Remove the 'unsup' directory as in the original code\n",
        "unsup_dir = extract_path / 'train' / 'unsup'\n",
        "if unsup_dir.exists():\n",
        "    shutil.rmtree(unsup_dir)\n",
        "    print(\"Removed 'unsup' directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVj-U7Ej9AgP"
      },
      "source": [
        "We will the train directory to create training and validation datasets with a split of 20% for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define parameters\n",
        "batch_size = 1024\n",
        "validation_split = 0.2\n",
        "seed = 123"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I917-Aci9AgP",
        "outputId": "f0c01a79-5be2-45d2-f1b9-070cac7d4005"
      },
      "outputs": [],
      "source": [
        "# Custom dataset class to load data from directories\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data_dir, subset='train'):\n",
        "        self.texts = []\n",
        "        self.labels = []\n",
        "        for label in ['pos', 'neg']:\n",
        "            labeled_dir = data_dir / subset / label\n",
        "            for file_path in labeled_dir.iterdir():\n",
        "                with open(file_path, encoding='utf-8') as f:\n",
        "                    self.texts.append(f.read())\n",
        "                    self.labels.append(1 if label == 'pos' else 0)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "# Load training data\n",
        "full_dataset = IMDBDataset(extract_path, 'train')\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_size = int((1 - validation_split) * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size],\n",
        "                                          generator=torch.Generator().manual_seed(seed))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNZ7im8a9AgP"
      },
      "source": [
        "Take a look at a few movie reviews and their labels (1: positive, 0: negative) from the train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5ucBWCo9AgQ",
        "outputId": "66685009-71ae-4f7c-89c9-d041763caecc",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 ....after 16 years Tim Burton finally disappoints me!!!! Whatever happened to the old Burton who rea\n",
            "1 Star Trek: Hidden Frontier is a long-running internet only fan film, done completely for the love of\n",
            "1 This is a romantic comedy with the emphasis on comedy for a change. As usual the lovers--Sally Field\n"
          ]
        }
      ],
      "source": [
        "for i in range(3):\n",
        "    print(full_dataset[i][1], full_dataset[i][0][:100])  # Print first 100 characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abL8GDrL9AgQ"
      },
      "source": [
        "## Using the Embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-VjLyXb9AgR"
      },
      "source": [
        "Next, define the dataset preprocessing steps required for your sentiment classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "psIeOSnS9AgR"
      },
      "outputs": [],
      "source": [
        "# Define a simple tokenizer (split by space)\n",
        "def tokenizer(text):\n",
        "    return text.split()\n",
        "\n",
        "# Define text preprocessing function\n",
        "def preprocess(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove HTML break tags\n",
        "    text = re.sub(r'<br\\s*/?>', ' ', text)\n",
        "    # Remove punctuation\n",
        "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
        "    return text\n",
        "\n",
        "# Build vocabulary\n",
        "def build_vocab(dataset, tokenizer, max_tokens):\n",
        "    freq = {}\n",
        "    for text, _ in dataset:\n",
        "        tokens = tokenizer(preprocess(text))\n",
        "        for token in tokens:\n",
        "            freq[token] = freq.get(token, 0) + 1\n",
        "    # Sort tokens by frequency\n",
        "    sorted_tokens = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
        "    # Limit to max_tokens\n",
        "    sorted_tokens = sorted_tokens[:max_tokens - 2]  # Reserve spots for <pad> and <unk>\n",
        "    # Create word to index mapping\n",
        "    vocab = {'<pad>': 0, '<unk>': 1}\n",
        "    for idx, (word, _) in enumerate(sorted_tokens, start=2):\n",
        "        vocab[word] = idx\n",
        "    return vocab\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 10000\n",
        "sequence_length = 100\n",
        "\n",
        "# Build vocabulary from training data\n",
        "vocab = build_vocab(train_dataset, tokenizer, vocab_size)\n",
        "inverse_vocab = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "# Function to numericalize and pad/truncate sequences\n",
        "def numericalize(text, vocab, tokenizer, seq_length):\n",
        "    tokens = tokenizer(preprocess(text))\n",
        "    numerical = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
        "    if len(numerical) < seq_length:\n",
        "        numerical += [vocab['<pad>']] * (seq_length - len(numerical))\n",
        "    else:\n",
        "        numerical = numerical[:seq_length]\n",
        "    return numerical\n",
        "\n",
        "# Define a collate function for DataLoader\n",
        "def collate_batch(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    numericalized = [numericalize(text, vocab, tokenizer, sequence_length) for text in texts]\n",
        "    padded = torch.tensor(numericalized, dtype=torch.long)\n",
        "    labels = torch.tensor(labels, dtype=torch.float32)\n",
        "    return padded, labels\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                          collate_fn=collate_batch, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                        collate_fn=collate_batch, num_workers=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0 [325, 1669, 9213, 5545, 345, 35, 4912, 2, 1, 1290, 7188, 1975, 3, 1, 4375, 1, 427, 3067, 3011, 3, 5867, 710, 9743, 16, 4950, 885, 538, 2317, 2866, 3, 49, 282, 7539, 1, 8, 550, 12, 289, 8, 195, 64, 197, 8528, 29, 21, 792, 609, 48, 2, 8528, 7, 31, 42, 18, 306, 1144, 9, 8, 2, 3007, 1, 936, 5, 1, 30, 2, 440, 18, 2, 101, 1, 57, 14, 2, 19, 4369, 962, 35, 12, 7, 2, 1994, 1, 5, 2, 135, 8, 1, 7188, 2, 959, 101, 1273, 4140, 361, 757, 5, 2773, 4, 396]\n",
            "1.0 [243, 9, 13, 73, 316, 3, 90, 51, 73, 316, 3, 90, 18, 2, 62, 13, 40, 37, 778, 3, 2, 1101, 5, 3330, 13, 53, 6054, 21, 12, 1342, 4586, 269, 49, 18, 2, 108, 27, 252, 13, 157, 18, 4, 1, 80, 22, 378, 6, 366, 69, 12, 3330, 13, 4, 2012, 5886, 1451, 2411, 16, 56, 1120, 5, 2, 1355, 41, 2, 2690, 10, 378, 36, 2, 544, 13, 27, 408, 6, 26, 3192, 1, 10, 378, 207, 20, 522, 40, 652, 22, 1, 92, 10, 375, 12, 2176, 13, 164, 15, 83, 158, 635, 233, 18]\n",
            "1.0 [11, 17, 7, 158, 3, 4151, 15, 99, 605, 9, 7, 388, 1, 715, 2, 175, 121, 4, 459, 285, 385, 1491, 8, 2, 1, 512, 5, 197, 9133, 1, 114, 71, 220, 2200, 157, 120, 565, 230, 2, 2185, 1, 5, 2, 2041, 7, 2, 160, 3, 61, 6194, 1, 35, 2, 1, 2, 1553, 6605, 14, 5368, 346, 3, 465, 2326, 15, 2, 4729, 5, 682, 32, 7225, 18, 128, 7225, 23, 465, 2713, 455, 107, 1, 16, 33, 1, 5, 5368, 7533, 3, 2448, 2790, 598, 28, 278, 20, 1, 2, 7225, 98, 2, 7225, 7903, 64]\n"
          ]
        }
      ],
      "source": [
        "# Preview some batches\n",
        "for texts, labels in train_loader:\n",
        "    for i in range(3):\n",
        "        print(labels[i].item(), texts[i].tolist())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPYRYxX69AgR"
      },
      "source": [
        "## Create a classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MsgfD3dM9AgR"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "class SentimentModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, output_dim=1):\n",
        "        super(SentimentModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab['<pad>'])\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc1 = nn.Linear(embedding_dim, 16)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(16, output_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
        "        embedded = embedded.permute(0, 2, 1)  # (batch_size, embedding_dim, seq_length)\n",
        "        pooled = self.global_avg_pool(embedded).squeeze(2)  # (batch_size, embedding_dim)\n",
        "        out = self.fc1(pooled)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out.squeeze(1)  # (batch_size)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "embedding_dim = 16\n",
        "model = SentimentModel(vocab_size, embedding_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uN6qmYzT9AgR"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__o8d2bw9AgS",
        "outputId": "05495aa1-66f9-4dac-b979-d769d71b239b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "Train Loss: 0.7010, Train Acc: 0.5002\n",
            "Val Loss: 0.6985, Val Acc: 0.4992\n",
            "Epoch 2/15\n",
            "Train Loss: 0.6959, Train Acc: 0.5002\n",
            "Val Loss: 0.6945, Val Acc: 0.4988\n",
            "Epoch 3/15\n",
            "Train Loss: 0.6926, Train Acc: 0.5099\n",
            "Val Loss: 0.6918, Val Acc: 0.5260\n",
            "Epoch 4/15\n",
            "Train Loss: 0.6901, Train Acc: 0.5592\n",
            "Val Loss: 0.6897, Val Acc: 0.5724\n",
            "Epoch 5/15\n",
            "Train Loss: 0.6877, Train Acc: 0.5802\n",
            "Val Loss: 0.6870, Val Acc: 0.5920\n",
            "Epoch 6/15\n",
            "Train Loss: 0.6842, Train Acc: 0.6038\n",
            "Val Loss: 0.6829, Val Acc: 0.6066\n",
            "Epoch 7/15\n",
            "Train Loss: 0.6787, Train Acc: 0.6228\n",
            "Val Loss: 0.6765, Val Acc: 0.6266\n",
            "Epoch 8/15\n",
            "Train Loss: 0.6701, Train Acc: 0.6472\n",
            "Val Loss: 0.6669, Val Acc: 0.6484\n",
            "Epoch 9/15\n",
            "Train Loss: 0.6575, Train Acc: 0.6644\n",
            "Val Loss: 0.6530, Val Acc: 0.6674\n",
            "Epoch 10/15\n",
            "Train Loss: 0.6397, Train Acc: 0.6914\n",
            "Val Loss: 0.6348, Val Acc: 0.6822\n",
            "Epoch 11/15\n",
            "Train Loss: 0.6170, Train Acc: 0.7058\n",
            "Val Loss: 0.6125, Val Acc: 0.6982\n",
            "Epoch 12/15\n",
            "Train Loss: 0.5899, Train Acc: 0.7251\n",
            "Val Loss: 0.5875, Val Acc: 0.7126\n",
            "Epoch 13/15\n",
            "Train Loss: 0.5609, Train Acc: 0.7426\n",
            "Val Loss: 0.5626, Val Acc: 0.7274\n",
            "Epoch 14/15\n",
            "Train Loss: 0.5321, Train Acc: 0.7597\n",
            "Val Loss: 0.5398, Val Acc: 0.7442\n",
            "Epoch 15/15\n",
            "Train Loss: 0.5051, Train Acc: 0.7740\n",
            "Val Loss: 0.5190, Val Acc: 0.7548\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "epochs = 15\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    for texts, labels in train_loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item() * texts.size(0)\n",
        "        preds = torch.round(torch.sigmoid(outputs))\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_samples += texts.size(0)\n",
        "    \n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = total_correct / total_samples\n",
        "    \n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in val_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            val_loss += loss.item() * texts.size(0)\n",
        "            preds = torch.round(torch.sigmoid(outputs))\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_samples += texts.size(0)\n",
        "    \n",
        "    avg_val_loss = val_loss / val_samples\n",
        "    val_accuracy = val_correct / val_samples\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    print(f\"Train Loss: {avg_loss:.4f}, Train Acc: {accuracy:.4f}\")\n",
        "    print(f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5SLPwFZ9AgS",
        "outputId": "060941df-0bcf-4262-9efb-ab4daea3e6b2",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Summary:\n",
            "SentimentModel(\n",
            "  (embedding): Embedding(10000, 16, padding_idx=0)\n",
            "  (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "  (fc1): Linear(in_features=16, out_features=16, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Display model summary\n",
        "print(\"\\nModel Summary:\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twdRkCtx9AgS"
      },
      "source": [
        "## Retrieve the trained word embeddings and save them to disk\n",
        "\n",
        "Next, retrieve the word embeddings learned during training. The embeddings are weights of the Embedding layer in the model. The weights matrix is of shape `(vocab_size, embedding_dimension)`.\n",
        "\n",
        "Obtain the weights from the model using `get_layer()` and `get_weights()`. The `get_vocabulary()` function provides the vocabulary to build a metadata file with one token per line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "65YToiqh9AgS"
      },
      "outputs": [],
      "source": [
        "# Get embedding weights\n",
        "embedding_weights = model.embedding.weight.data.cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raYipkidP1Up"
      },
      "source": [
        "Write the weights to disk. To use the [Embedding Projector](http://projector.tensorflow.org), you will upload two files in tab separated format: a file of vectors (containing the embedding), and a file of meta data (containing the words)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "i0L33NJ89AgS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings and metadata have been saved.\n"
          ]
        }
      ],
      "source": [
        "# Save embeddings to 'vectors.tsv' and 'metadata.tsv'\n",
        "with open('vectors.tsv', 'w', encoding='utf-8') as out_v, \\\n",
        "     open('metadata.tsv', 'w', encoding='utf-8') as out_m:\n",
        "    for idx, word in enumerate(inverse_vocab):\n",
        "        if idx == vocab['<pad>']:\n",
        "            continue  # skip padding\n",
        "        vec = embedding_weights[idx]\n",
        "        out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "        out_m.write(str(word) + \"\\n\")\n",
        "\n",
        "print(\"Embeddings and metadata have been saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptLFvo4d9AgS"
      },
      "source": [
        "## Bag of Words (BoW)\n",
        "\n",
        "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
        "\n",
        "1. A vocabulary of known words.\n",
        "2. A measure of the presence of known words.\n",
        "\n",
        "### Step 1: Collect Data\n",
        "\n",
        "    It was the best of times,\n",
        "    it was the worst of times,\n",
        "    it was the age of wisdom,\n",
        "    it was the age of foolishness,\n",
        "\n",
        "For this small example, let’s treat each line as a separate “document” and the 4 lines as our entire corpus of documents.\n",
        "\n",
        "### Step 2: Design the Vocabulary\n",
        "Now we can make a list of all of the words in our model vocabulary.\n",
        "\n",
        "The unique words here (ignoring case and punctuation) are:\n",
        "\n",
        "“it”\n",
        "\n",
        "“was”\n",
        "\n",
        "“the”\n",
        "\n",
        "“best”\n",
        "\n",
        "“of”\n",
        "\n",
        "“times”\n",
        "\n",
        "“worst”\n",
        "\n",
        "“age”\n",
        "\n",
        "“wisdom”\n",
        "\n",
        "“foolishness”\n",
        "\n",
        "That is a vocabulary of 10 words from a corpus containing 24 words.\n",
        "\n",
        "### Step 3: Create Document Vectors\n",
        "The next step is to score the words in each document.\n",
        "\n",
        "The objective is to turn each document of free text into a vector that we can use as input or output for a machine learning model.\n",
        "\n",
        "Because we know the vocabulary has 10 words, we can use a fixed-length document representation of 10, with one position in the vector to score each word.\n",
        "\n",
        "The simplest scoring method is to mark the presence of words as a boolean value, 0 for absent, 1 for present.\n",
        "\n",
        "Using the arbitrary ordering of words listed above in our vocabulary, we can step through the first document (“It was the best of times“) and convert it into a binary vector.\n",
        "\n",
        "The scoring of the document would look as follows:\n",
        "\n",
        "“it” = 1\n",
        "\n",
        "“was” = 1\n",
        "\n",
        "“the” = 1\n",
        "\n",
        "“best” = 1\n",
        "\n",
        "“of” = 1\n",
        "\n",
        "“times” = 1\n",
        "\n",
        "“worst” = 0\n",
        "\n",
        "“age” = 0\n",
        "\n",
        "“wisdom” = 0\n",
        "\n",
        "“foolishness” = 0\n",
        "\n",
        "As a binary vector, this would look as follows:\n",
        "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
        "\n",
        "The other three documents would look as follows:\n",
        "\n",
        "\"it was the worst of times\" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
        "\n",
        "\"it was the age of wisdom\" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
        "\n",
        "\"it was the age of foolishness\" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n",
        "\n",
        "\n",
        "#### Practice what words are most likely to appear in a spam email?\n",
        "\n",
        "### Limitations\n",
        "\n",
        "- **Vocabulary**: How many words in English?\n",
        "\n",
        "- **Sparsity**: (0,1) hard to model.\n",
        "\n",
        "- **Meaning**: Discarding word order ignores the context, and in turn meaning of words in the document (semantics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDLVk1bh9AgT"
      },
      "source": [
        "## Word2Vec Algorithem\n",
        "Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through Word2Vec have proven to be successful on a variety of downstream natural language processing tasks.\n",
        "\n",
        "Efficient Estimation of Word Representations in\n",
        "Vector Space https://arxiv.org/pdf/1301.3781.pdf\n",
        "\n",
        "Distributed Representations of Words and Phrases\n",
        "and their Compositionality https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n",
        "\n",
        "These papers proposed two methods for learning representations of words:\n",
        "\n",
        "- Continuous Bag-of-Words Model which predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.\n",
        "- Continuous Skip-gram Model which predict words within a certain range before and after the current word in the same sentence. A worked example of this is given below.\n",
        "\n",
        "You'll use the skip-gram approach in this tutorial. First, you'll explore skip-grams and other concepts using a single sentence for illustration. Next, you'll train your own Word2Vec model on a small dataset.\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*xD9n3KeWXuenMNL_BpYp6A.png\" alt=\"Diagram of one-hot encodings\" width=\"600\" />\n",
        "\n",
        "source(medium.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WFPd5Z-9AgT"
      },
      "source": [
        "## Skip-gram and Negative Sampling\n",
        "While a bag-of-words model predicts a word given the neighboring context, a skip-gram model predicts the context (or neighbors) of a word, given the word itself. The model is trained on skip-grams, which are n-grams that allow tokens to be skipped (see the diagram below for an example). The context of a word can be represented through a set of skip-gram pairs of (target_word, context_word) where context_word appears in the neighboring context of target_word.\n",
        "\n",
        "\n",
        "Consider the following sentence of 8 words.\n",
        "> The wide road shimmered in the hot sun.\n",
        "\n",
        "The context words for each of the 8 words of this sentence are defined by a window size. The window size determines the span of words on either side of a `target_word` that can be considered `context word`. Take a look at this table of skip-grams for target words based on different window siz\n",
        "\n",
        "Note: For this tutorial, a window size of *n* implies n words on each side with a total window span of 2*n+1 words across a word.\n",
        "\n",
        "![word2vec_skipgrams](https://tensorflow.org/text/tutorials/images/word2vec_skipgram.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIDKDOXX9AgT"
      },
      "source": [
        "The training objective of the skip-gram model is to maximize the probability of predicting context words given the target word. For a sequence of words w1, w2, ... wT, the objective can be written as the average log probability\n",
        "\n",
        "\n",
        "![word2vec_skipgram_objective](https://tensorflow.org/text/tutorials/images/word2vec_skipgram_objective.png)\n",
        "\n",
        "\n",
        "where c is the size of the training context. The basic skip-gram formulation defines this probability using the softmax function.\n",
        "\n",
        "![word2vec_full_softmax](https://tensorflow.org/text/tutorials/images/word2vec_full_softmax.png)\n",
        "\n",
        "\n",
        "where v and v' are target and context vector representations of words and W is vocabulary size.\n",
        "\n",
        "Computing the denominator of this formulation involves performing a full softmax over the entire vocabulary words which is often large (105-107) terms.\n",
        "\n",
        "The Noise Contrastive Estimation loss function is an efficient approximation for a full softmax. With an objective to learn word embeddings instead of modelling the word distribution, NCE loss can be simplified to use negative sampling.\n",
        "\n",
        "The simplified negative sampling objective for a target word is to distinguish the context word from num_ns negative samples drawn from noise distribution Pn(w) of words. More precisely, an efficient approximation of full softmax over the vocabulary is, for a skip-gram pair, to pose the loss for a target word as a classification problem between the context word and num_ns negative samples.\n",
        "\n",
        "A negative sample is defined as a (target_word, context_word) pair such that the context_word does not appear in the window_size neighborhood of the target_word. For the example sentence, these are few potential negative samples (when window_size is 2).\n",
        "\n",
        "\n",
        "(hot, shimmered)\n",
        "(wide, hot)\n",
        "(wide, sun)\n",
        "\n",
        "In the next section, you'll generate skip-grams and negative samples for a single sentence. You'll also learn about subsampling techniques and train a classification model for positive and negative training examples later in the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g6UvMUI9Agb"
      },
      "source": [
        "### vectorize an example sentence\n",
        "Consider the following sentence:\n",
        "The wide road shimmered in the hot sun.\n",
        "\n",
        "Tokenize the sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5AVX2WB9Agb",
        "outputId": "909d67a9-4244-4bd9-ce1b-abba3c478e8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n"
          ]
        }
      ],
      "source": [
        "# Define a simple sentence\n",
        "sentence = \"The wide road shimmered in the hot sun\"\n",
        "tokens = sentence.lower().split()\n",
        "print(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFT_C9M69Agb",
        "outputId": "696c4e78-01ac-4f88-c84c-599c921996ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Build vocabulary manually\n",
        "manual_vocab = {'<pad>':0}\n",
        "index = 1\n",
        "for token in tokens:\n",
        "    if token not in manual_vocab:\n",
        "        manual_vocab[token] = index\n",
        "        index += 1\n",
        "vocab_size_manual = len(manual_vocab)\n",
        "print(manual_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tlwj5Pgw9Agc",
        "outputId": "535b6b02-00c1-4700-9660-d966a8a2f36d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"
          ]
        }
      ],
      "source": [
        "# Create inverse vocabulary\n",
        "inverse_vocab_manual = {idx: word for word, idx in manual_vocab.items()}\n",
        "print(inverse_vocab_manual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEyeiV0H9Agc",
        "outputId": "a30380b0-b145-4fc6-a255-059dc23bf776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 1, 6, 7]\n"
          ]
        }
      ],
      "source": [
        "# Vectorize the sentence\n",
        "example_sequence = [manual_vocab[word] for word in tokens]\n",
        "print(example_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3QAceJb9Agc"
      },
      "source": [
        "### Generating skip-grams from one sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCfkFsEF9Agc",
        "outputId": "6e76b1db-9523-450e-e375-f9b28f529110"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26\n"
          ]
        }
      ],
      "source": [
        "# Define window size\n",
        "window_size = 2\n",
        "\n",
        "# Generate positive skip-grams\n",
        "def generate_skipgrams(sequence, window_size):\n",
        "    skip_grams = []\n",
        "    for i, target in enumerate(sequence):\n",
        "        context_start = max(0, i - window_size)\n",
        "        context_end = min(len(sequence), i + window_size + 1)\n",
        "        for j in range(context_start, context_end):\n",
        "            if j != i:\n",
        "                skip_grams.append((target, sequence[j]))\n",
        "    return skip_grams\n",
        "\n",
        "positive_skip_grams = generate_skipgrams(example_sequence, window_size)\n",
        "print(len(positive_skip_grams))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lwpaanA9Agc",
        "outputId": "c1b108b9-0f94-4e65-f679-65fa72c25995"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 2): (the, wide)\n",
            "(1, 3): (the, road)\n",
            "(2, 1): (wide, the)\n",
            "(2, 3): (wide, road)\n",
            "(2, 4): (wide, shimmered)\n"
          ]
        }
      ],
      "source": [
        "# Display some skip-grams\n",
        "for target, context in positive_skip_grams[:5]:\n",
        "    print(f\"({target}, {context}): ({inverse_vocab_manual[target]}, {inverse_vocab_manual[context]})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVUDngcw9Agc"
      },
      "source": [
        "### Negative sampling for one skip-gram\n",
        "\n",
        "The `skipgrams` function returns all positive skip-gram pairs by sliding over a given window span. To produce additional skip-gram pairs that would serve as negative samples for training, you need to sample random words from the vocabulary. Use the `tf.random.log_uniform_candidate_sampler` function to sample `num_ns` number of negative samples for a given target word in a window. You can call the function on one skip-grams's target word and pass the context word as true class to exclude it from being sampled.\n",
        "\n",
        "Key point: `num_ns` (the number of negative samples per a positive context word) in the `[5, 20]` range is [shown to work](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) best for smaller datasets, while `num_ns` in the `[2, 5]` range suffices for larger datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A6-8mKB9Agc",
        "outputId": "cad66323-1d2f-4ed1-9b71-a4179bf07c7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7, 4, 5, 3]\n",
            "['sun', 'shimmered', 'in', 'road']\n"
          ]
        }
      ],
      "source": [
        "# Select one positive skip-gram\n",
        "target_word, context_word = positive_skip_grams[0]\n",
        "\n",
        "# Number of negative samples\n",
        "num_ns = 4\n",
        "\n",
        "# Negative sampling using uniform distribution\n",
        "def negative_sampling(context_word, num_ns, vocab_size, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    negatives = []\n",
        "    while len(negatives) < num_ns:\n",
        "        neg = np.random.randint(1, vocab_size)  # skip 0 (padding)\n",
        "        if neg != context_word and neg not in negatives:\n",
        "            negatives.append(neg)\n",
        "    return negatives\n",
        "\n",
        "negative_samples = negative_sampling(context_word, num_ns, vocab_size_manual)\n",
        "print(negative_samples)\n",
        "print([inverse_vocab_manual[idx] for idx in negative_samples])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_x40Q_h9Agd"
      },
      "source": [
        "### Construct one training example\n",
        "For a given positive `(target_word, context_word)` skip-gram, you now also have `num_ns` negative sampled context words that do not appear in the window size neighborhood of `target_word`. Batch the `1` positive `context_word` and `num_ns` negative context words into one tensor. This produces a set of positive skip-grams (labeled as `1`) and negative samples (labeled as `0`) for each target word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "N5n79ReH9Agd"
      },
      "outputs": [],
      "source": [
        "# Prepare context and labels\n",
        "context = [context_word] + negative_samples\n",
        "labels = [1] + [0]*num_ns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWd936jH9Agd"
      },
      "source": [
        "Take a look at the context and the corresponding labels for the target word from the skip-gram example above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3TeeVQc9Agd",
        "outputId": "e8262cbc-e997-4718-a3e9-59d707d0d740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "target_index    : 1\n",
            "target_word     : the\n",
            "context_indices : [2, 7, 4, 5, 3]\n",
            "context_words   : ['wide', 'sun', 'shimmered', 'in', 'road']\n",
            "label           : [1, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "print(f\"target_index    : {target_word}\")\n",
        "print(f\"target_word     : {inverse_vocab_manual[target_word]}\")\n",
        "print(f\"context_indices : {context}\")\n",
        "print(f\"context_words   : {[inverse_vocab_manual[c] for c in context]}\")\n",
        "print(f\"label           : {labels}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4XPFIQY9Agd"
      },
      "source": [
        "A tuple of (target, context, label) tensors constitutes one training example for training your skip-gram negative sampling Word2Vec model. Notice that the target is of shape (1,) while the context and label are of shape (1+num_ns,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgIoJ-5x9Agd"
      },
      "source": [
        "### Summary\n",
        "\n",
        "This picture summarizes the procedure of generating training example from a sentence.\n",
        "\n",
        "![word2vec_negative_sampling](https://tensorflow.org/text/tutorials/images/word2vec_negative_sampling.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtu9wz3s9Agd"
      },
      "source": [
        "### Compile all steps into one function\n",
        "\n",
        "#### Skip-gram Sampling table\n",
        "A large dataset means larger vocabulary with higher number of more frequent words such as stopwords. Training examples obtained from sampling commonly occurring words (such as the, is, on) don't add much useful information for the model to learn from. Mikolov et al. suggest subsampling of frequent words as a helpful practice to improve embedding quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZj53UMa9Age",
        "outputId": "fc936889-e96a-486f-ad6a-c4155e5d8cca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7 4 6 9 2 6 7 4 3 7]\n"
          ]
        }
      ],
      "source": [
        "# Define a sampling table (not used in this simple example)\n",
        "sampling_table = np.random.randint(0, 10, size=10)\n",
        "print(sampling_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6QXJ4_aRYz8"
      },
      "source": [
        "`sampling_table[i]` denotes the probability of sampling the i-th most common word in a dataset. The function assumes a [Zipf's distribution](https://en.wikipedia.org/wiki/Zipf%27s_law) of the word frequencies for sampling.\n",
        "\n",
        "Key point: The `tf.random.log_uniform_candidate_sampler` already assumes that the vocabulary frequency follows a log-uniform (Zipf's) distribution. Using these distribution weighted sampling also helps approximate the Noise Contrastive Estimation (NCE) loss with simpler loss functions for training a negative sampling objective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv381Xt_9Age"
      },
      "source": [
        "### Generate training data\n",
        "\n",
        "Compile all the steps described above into a function that can be called on a list of vectorized sentences obtained from any text dataset. Notice that the sampling table is built before sampling skip-gram word pairs. You will use this function in the later sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "dd26lIwi9Age"
      },
      "outputs": [],
      "source": [
        "# Function to generate training data with negative sampling\n",
        "def generate_training_data_skipgram(sequences, window_size, num_ns, vocab_size, seed=42):\n",
        "    targets = []\n",
        "    contexts = []\n",
        "    labels = []\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    for sequence in sequences:\n",
        "        skip_grams = generate_skipgrams(sequence, window_size)\n",
        "        for target, context_word in skip_grams:\n",
        "            neg_samples = negative_sampling(context_word, num_ns, vocab_size, seed)\n",
        "            context_combined = [context_word] + neg_samples\n",
        "            label_combined = [1] + [0]*num_ns\n",
        "            targets.append(target)\n",
        "            contexts.append(context_combined)\n",
        "            labels.append(label_combined)\n",
        "    \n",
        "    return targets, contexts, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Lg1WMS49Age"
      },
      "source": [
        "### Prepare training data for Word2Vec\n",
        "\n",
        "With an understanding of how to work with one sentence for a skip-gram negative sampling based Word2Vec model, you can proceed to generate training examples from a larger list of sentences!\n",
        "\n",
        "#### Download text corpus\n",
        "You will use a text file of Shakespeare's writing for this tutorial. Change the following line to run this code on your own data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "dtunPw5q9Age"
      },
      "outputs": [],
      "source": [
        "# Example sequences (for demonstration, using the single sentence)\n",
        "sequences = [example_sequence]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFnCIdwv9Age",
        "outputId": "e78b3536-35d8-440d-d40c-f185549276eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "targets.shape: 26\n",
            "contexts.shape: 26\n",
            "labels.shape: 26\n"
          ]
        }
      ],
      "source": [
        "# Generate training data\n",
        "targets, contexts, labels = generate_training_data_skipgram(\n",
        "    sequences=sequences,\n",
        "    window_size=window_size,\n",
        "    num_ns=num_ns,\n",
        "    vocab_size=vocab_size_manual,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print('\\n')\n",
        "print(f\"targets.shape: {len(targets)}\")\n",
        "print(f\"contexts.shape: {len(contexts)}\")\n",
        "print(f\"labels.shape: {len(labels)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCEHRW-W9Agf"
      },
      "source": [
        "Vectorize sentences from the corpus\n",
        "\n",
        "You can use the TextVectorization layer to vectorize sentences from the corpus. Learn more about using this layer in this Text Classification tutorial. Notice from the first few sentences above that the text needs to be in one case and punctuation needs to be removed. To do this, define a custom_standardization function that can be used in the TextVectorization layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "1jwxWy_W9Agf"
      },
      "outputs": [],
      "source": [
        "# Define a custom Dataset for Word2Vec\n",
        "class Word2VecDataset(Dataset):\n",
        "    def __init__(self, targets, contexts, labels):\n",
        "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
        "        self.contexts = torch.tensor(contexts, dtype=torch.long)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return (self.targets[idx], self.contexts[idx]), self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "9QZUdid09Agf"
      },
      "outputs": [],
      "source": [
        "# Create the dataset and dataloader\n",
        "w2v_dataset = Word2VecDataset(targets, contexts, labels)\n",
        "w2v_loader = DataLoader(w2v_dataset, batch_size=1024, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEcOp2O-9Agf"
      },
      "source": [
        "Once the state of the layer has been adapted to represent the text corpus, the vocabulary can be accessed with get_vocabulary(). This function returns a list of all vocabulary tokens sorted (descending) by their frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaT44lNN9Agf",
        "outputId": "675f151c-b6e6-466f-9eaa-86de7fa9ed19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x72904ebc6380>\n"
          ]
        }
      ],
      "source": [
        "print(w2v_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB5TMxVy9Agg"
      },
      "source": [
        "### Model and Training\n",
        "The Word2Vec model can be implemented as a classifier to distinguish between true context words from skip-grams and false context words obtained through negative sampling. You can perform a dot product between the embeddings of target and context words to obtain predictions for labels and compute loss against true labels in the dataset.\n",
        "\n",
        "#### Subclassed Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "-WH-oQOo9Agh"
      },
      "outputs": [],
      "source": [
        "class Word2VecModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_ns):\n",
        "        super(Word2VecModel, self).__init__()\n",
        "        self.target_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.context_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.num_ns = num_ns\n",
        "    \n",
        "    def forward(self, pair):\n",
        "        target, context = pair\n",
        "        target = target.unsqueeze(1)  # (batch, 1)\n",
        "        word_emb = self.target_embedding(target)  # (batch, 1, embed_dim)\n",
        "        context_emb = self.context_embedding(context)  # (batch, context, embed_dim)\n",
        "        # Compute dot product between target and context embeddings\n",
        "        dots = torch.bmm(context_emb, word_emb.transpose(1, 2)).squeeze(2)  # (batch, context)\n",
        "        return dots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "eEIAYUl29Agh"
      },
      "outputs": [],
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "embedding_dim_w2v = 128\n",
        "word2vec_model = Word2VecModel(vocab_size_manual, embedding_dim_w2v, num_ns).to(device)\n",
        "criterion_w2v = nn.BCEWithLogitsLoss()\n",
        "optimizer_w2v = optim.Adam(word2vec_model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNBnA7B19Agh",
        "outputId": "9977cae0-26d1-4345-c4b2-6eaba403475f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 1.1611, Train Acc: 0.3692\n",
            "Epoch 2/20\n",
            "Train Loss: 1.1496, Train Acc: 0.3692\n",
            "Epoch 3/20\n",
            "Train Loss: 1.1382, Train Acc: 0.3692\n",
            "Epoch 4/20\n",
            "Train Loss: 1.1269, Train Acc: 0.4000\n",
            "Epoch 5/20\n",
            "Train Loss: 1.1156, Train Acc: 0.4000\n",
            "Epoch 6/20\n",
            "Train Loss: 1.1043, Train Acc: 0.4000\n",
            "Epoch 7/20\n",
            "Train Loss: 1.0932, Train Acc: 0.4000\n",
            "Epoch 8/20\n",
            "Train Loss: 1.0821, Train Acc: 0.4000\n",
            "Epoch 9/20\n",
            "Train Loss: 1.0710, Train Acc: 0.4000\n",
            "Epoch 10/20\n",
            "Train Loss: 1.0600, Train Acc: 0.4000\n",
            "Epoch 11/20\n",
            "Train Loss: 1.0490, Train Acc: 0.4000\n",
            "Epoch 12/20\n",
            "Train Loss: 1.0381, Train Acc: 0.4000\n",
            "Epoch 13/20\n",
            "Train Loss: 1.0273, Train Acc: 0.4000\n",
            "Epoch 14/20\n",
            "Train Loss: 1.0165, Train Acc: 0.4000\n",
            "Epoch 15/20\n",
            "Train Loss: 1.0057, Train Acc: 0.4077\n",
            "Epoch 16/20\n",
            "Train Loss: 0.9950, Train Acc: 0.4077\n",
            "Epoch 17/20\n",
            "Train Loss: 0.9844, Train Acc: 0.4308\n",
            "Epoch 18/20\n",
            "Train Loss: 0.9738, Train Acc: 0.4308\n",
            "Epoch 19/20\n",
            "Train Loss: 0.9632, Train Acc: 0.4308\n",
            "Epoch 20/20\n",
            "Train Loss: 0.9527, Train Acc: 0.4462\n"
          ]
        }
      ],
      "source": [
        "# Training loop for Word2Vec\n",
        "epochs_w2v = 20\n",
        "for epoch in range(epochs_w2v):\n",
        "    word2vec_model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    for (targets_batch, contexts_batch), labels_batch in w2v_loader:\n",
        "        targets_batch = targets_batch.to(device)\n",
        "        contexts_batch = contexts_batch.to(device)\n",
        "        labels_batch = labels_batch.to(device)\n",
        "        \n",
        "        optimizer_w2v.zero_grad()\n",
        "        outputs = word2vec_model((targets_batch, contexts_batch))\n",
        "        loss = criterion_w2v(outputs, labels_batch)\n",
        "        loss.backward()\n",
        "        optimizer_w2v.step()\n",
        "        \n",
        "        total_loss += loss.item() * targets_batch.size(0)\n",
        "        preds = torch.round(torch.sigmoid(outputs))\n",
        "        total_correct += (preds == labels_batch).sum().item()\n",
        "        total_samples += labels_batch.numel()\n",
        "    \n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = total_correct / total_samples\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{epochs_w2v}\")\n",
        "    print(f\"Train Loss: {avg_loss:.4f}, Train Acc: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Z-FMov9Agi"
      },
      "source": [
        "### Embedding lookup and analysis\n",
        "Obtain the weights from the model using get_layer() and get_weights(). The get_vocabulary() function provides the vocabulary to build a metadata file with one token per line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "XVBk1Lm59Agi"
      },
      "outputs": [],
      "source": [
        "# Get Word2Vec embedding weights\n",
        "w2v_weights = word2vec_model.target_embedding.weight.data.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "vRALOxw_9Agi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec embeddings and metadata have been saved.\n"
          ]
        }
      ],
      "source": [
        "# Save embeddings to 'vectorsw2v.tsv' and 'metadataw2v.tsv'\n",
        "with open('vectorsw2v.tsv', 'w', encoding='utf-8') as out_v, \\\n",
        "     open('metadataw2v.tsv', 'w', encoding='utf-8') as out_m:\n",
        "    for idx, word in inverse_vocab_manual.items():\n",
        "        if idx == manual_vocab['<pad>']:\n",
        "            continue  # skip padding\n",
        "        vec = w2v_weights[idx]\n",
        "        out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "        out_m.write(word + \"\\n\")\n",
        "\n",
        "print(\"Word2Vec embeddings and metadata have been saved.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
